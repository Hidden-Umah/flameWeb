# Minimal NLP (from Scratch)

ğŸš€ An educational project to learn and build **Natural Language Processing (NLP)** models from scratch using **Python + NumPy only** (no high-level ML libraries).  
The goal is to understand the **core building blocks** of NLP without relying on frameworks like GPT, BERT, or TensorFlow.  

---

## ğŸ“Œ Project Overview
This project is part of my journey to learn **AI and NLP fundamentals** by implementing everything myself.  
Weâ€™ll start small (tokenization, vocabulary building, encoding) and progress toward simple models (bag-of-words, embeddings, RNNs).  

---

## âœ¨ Features (Planned / In Progress)
- âœ… Basic text cleaning and tokenization  
- âœ… Vocabulary building & word-to-index mapping  
- â³ Bag-of-Words representation  
- â³ TF-IDF implementation  
- â³ Word embeddings (from scratch)  
- â³ Simple RNN for text classification  

---

## ğŸ“‚ Project Structure

minimal-nlp/
â”‚â”€â”€ data/ # Sample text data
â”‚â”€â”€ src/ # Source code (tokenizers, models, utils)
â”‚â”€â”€ notebooks/ # Jupyter notebooks (experiments, demos)
â”‚â”€â”€ tests/ # Unit tests
â”‚â”€â”€ README.md # Project documentation
---

## ğŸ› ï¸ Tech Stack
- **Python 3.10+**
- **NumPy**
- (Optional later: Matplotlib for visualization)

---

### ğŸš€ Getting Started
Clone the repo:
```bash
git clone https://github.com/Hidden-Umah/Legend-ai-lab.git
cd Legend-ai-lab

Install dependencies:

pip install -r requirements.txt

Run a simple tokenizer demo:

python src/tokenizer.py

ğŸ“… Roadmap

    Week 1 â†’ Tokenization & Vocabulary

    Week 2 â†’ Bag-of-Words & TF-IDF

    Week 3 â†’ Word Embeddings

    Week 4 â†’ Basic RNN for text classification

ğŸ¤ Contributing

This is mainly a learning project, but feedback and suggestions are welcome!
ğŸ“– License

MIT License â€” free to use, learn, and share.


---

